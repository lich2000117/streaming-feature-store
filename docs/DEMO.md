# ğŸ¯ Complete Demo Walkthrough

This guide provides a comprehensive walkthrough of the streaming feature store platform, showcasing all capabilities from data ingestion to real-time inference.

## ğŸš€ Phase 1: Platform Launch

### Start Core Infrastructure
```bash
# Launch message broker, feature store, and ML infrastructure
make up
```

**What happens:**
- âœ… Kafka/Redpanda message broker starts
- âœ… Redis feature store initializes
- âœ… Schema Registry configures data contracts
- âœ… MLflow tracking server launches

### Verify Infrastructure Health
```bash
make health
```

Expected output:
```
ğŸ¥ Health Check
ğŸ“Š Kafka: âœ… Healthy
ğŸ—„ï¸ Redis: âœ… Healthy  
ğŸ“‹ Schema Registry: âœ… Healthy
ğŸ¤– MLflow: âœ… Healthy
```

## ğŸ“Š Phase 2: Data Generation

### Start Event Generators
```bash
make generate
```

**Real-time event streams:**
- **Transaction Generator**: 10 events/sec with 40% fraud injection
- **Clickstream Generator**: 4 events/sec with user session tracking

### Monitor Data Flow
```bash
make inspect
```

You'll see live metrics:
```
ğŸ“ˆ Live Data Flow:
â”œâ”€â”€ Transaction Events: ~600/min
â”œâ”€â”€ Click Events: ~240/min  
â”œâ”€â”€ Fraud Rate: ~40%
â””â”€â”€ Active Sessions: ~150
```

## âš¡ Phase 3: Stream Processing

### Launch Feature Engineering
```bash
make stream
```

**Real-time feature computation:**
- Transaction aggregations (1min, 5min, 1hour windows)
- User behavior patterns
- Fraud risk indicators
- Session analytics

### Verify Feature Store Population
```bash
make inspect
```

Features being computed:
```
ğŸ” Feature Store Status:
â”œâ”€â”€ Transaction Features: âœ… Active
â”œâ”€â”€ User Engagement: âœ… Active
â”œâ”€â”€ Risk Indicators: âœ… Active
â””â”€â”€ Session Metrics: âœ… Active
```

## ğŸ¤– Phase 4: ML Training

### Train Initial Models
```bash
make train
```

**Training pipeline:**
- Fraud detection model (XGBoost)
- Personalization model (LightGBM)
- Model validation and registration
- ONNX export for serving

### Enable Automated Retraining
```bash
make train-scheduled
```

Models retrain every 10 minutes with fresh data.

## ğŸš€ Phase 5: Online Inference

### Start Inference API
```bash
make serve
```

**FastAPI service features:**
- Sub-150ms p95 latency
- Async feature fetching
- Model serving with ONNX
- Comprehensive metrics

### Test API Performance
```bash
make test-api
```

Performance results:
```
âš¡ API Performance Test:
â”œâ”€â”€ Latency p50: ~45ms
â”œâ”€â”€ Latency p95: ~120ms
â”œâ”€â”€ Throughput: ~8k requests/sec
â””â”€â”€ Error Rate: 0%
```

## ğŸ“ˆ Phase 6: Monitoring & Observability

### Launch Monitoring Stack
```bash
make monitor
```

**Complete observability:**
- Prometheus metrics collection
- Grafana dashboards
- Real-time alerting
- Performance tracking

### Access Dashboards

| Dashboard | URL | Purpose |
|:----------|:----|:--------|
| **Fraud Detection** | [localhost:3000](http://localhost:3000) | Live fraud rates, blocked transactions |
| **MLflow Experiments** | [localhost:5001](http://localhost:5001) | Model training and versioning |
| **System Metrics** | [localhost:9090](http://localhost:9090) | Infrastructure performance |

## ğŸ¯ Phase 7: End-to-End Demo

### Complete Platform Launch
```bash
make demo
```

This single command launches everything:
- âœ… Core infrastructure
- âœ… Event generation 
- âœ… Stream processing
- âœ… ML inference
- âœ… Monitoring stack

### Demonstrate Key Use Cases

#### 1. Real-Time Fraud Detection
```bash
# Send test transaction
curl -X POST "http://localhost:8080/score/fraud" \
  -H "Content-Type: application/json" \
  -d '{
    "transaction_id": "test-123",
    "card_id": "card-456", 
    "amount": 1500.00,
    "merchant_category": "electronics"
  }'
```

Response:
```json
{
  "fraud_score": 0.85,
  "risk_level": "HIGH", 
  "decision": "BLOCK",
  "latency_ms": 89
}
```

#### 2. Personalization Scoring
```bash
# Get user propensity score
curl -X POST "http://localhost:8080/score/personalization" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user-789",
    "item_category": "electronics",
    "context": {"time_of_day": "evening"}
  }'
```

Response:
```json
{
  "propensity_score": 0.72,
  "recommendation": "SHOW_OFFER",
  "confidence": 0.89,
  "latency_ms": 67
}
```

## ğŸ“Š Performance Verification

### System Health Check
```bash
make health
```

All services should be healthy:
```
âœ… Kafka: Healthy (3 partitions, 0 lag)
âœ… Redis: Healthy (150k+ features cached)
âœ… Stream Processor: Healthy (8k events/sec)
âœ… Inference API: Healthy (120ms p95)
âœ… MLflow: Healthy (2 models registered)
```

### Load Testing
```bash
# Generate high load
make test-latency
```

Performance under load:
```
ğŸ“ˆ Load Test Results (1000 concurrent):
â”œâ”€â”€ Average Latency: 78ms
â”œâ”€â”€ p95 Latency: 145ms
â”œâ”€â”€ p99 Latency: 234ms
â”œâ”€â”€ Throughput: 7.2k RPS
â””â”€â”€ Success Rate: 99.8%
```

## ğŸ” Troubleshooting

### Common Issues

**Issue**: Kafka connection errors
```bash
# Check Kafka health
docker logs kafka --tail 20
make health
```

**Issue**: High inference latency
```bash
# Check Redis connection
redis-cli ping
# Monitor feature store metrics
make logs-api
```

**Issue**: Model training failures
```bash
# Check MLflow logs
make logs-ml
# Verify data availability
make inspect
```

### Performance Tuning

**Optimize for Higher Throughput:**
```bash
# Increase generator rates
docker-compose -f infra/docker-compose.yml up -d \
  --scale txn-generator=3 \
  --scale click-generator=2
```

**Optimize for Lower Latency:**
```bash
# Tune Redis for speed
redis-cli CONFIG SET maxmemory-policy allkeys-lru
# Enable connection pooling
export REDIS_POOL_SIZE=20
```

## ğŸ¯ Demo Script for Presentations

### 30-Second Demo
```bash
# 1. Launch everything (5 seconds)
make demo

# 2. Show real-time dashboards (10 seconds)
open http://localhost:3000

# 3. Test API performance (10 seconds)
make test-api

# 4. Show MLflow experiments (5 seconds)
open http://localhost:5001
```

### Key Talking Points

1. **"This is a production-grade streaming ML platform"**
   - Show docker-compose.yml with profiles
   - Highlight health checks and monitoring

2. **"Sub-150ms fraud detection at scale"**
   - Demonstrate API latency tests
   - Show Grafana performance dashboards

3. **"Complete MLOps lifecycle"**
   - Show MLflow experiment tracking
   - Demonstrate automated retraining

4. **"Cloud-ready architecture"**
   - Explain containerization strategy
   - Discuss scaling and deployment options

## ğŸš€ Next Steps

1. **Scale Testing**: Increase load to test horizontal scaling
2. **Feature Development**: Add new features using the feature store
3. **Model Experimentation**: Try different algorithms in MLflow
4. **Cloud Deployment**: Deploy to AWS/GCP using the migration guide
5. **Custom Use Cases**: Adapt the platform for your specific domain

This platform demonstrates senior-level capabilities in:
- **Stream Processing**: Real-time data pipelines
- **MLOps**: Complete model lifecycle management  
- **Infrastructure**: Production-ready deployment
- **Observability**: Comprehensive monitoring and alerting
